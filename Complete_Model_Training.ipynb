{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3adcf21",
   "metadata": {},
   "source": [
    "# Model Training on TCGA + CGGA Data for shipping in pyGSLModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93608fb8",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2424c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.base import clone\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.statistics import logrank_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79bb42",
   "metadata": {},
   "source": [
    "### Setting up the ANN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77ea6891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### Defining Helper Classes for ANN ###\n",
    "#######################################\n",
    "\n",
    "# Focal Loss class for enabling training focused on the difficult to predict class\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=5, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        targets = targets.view(-1,1).type_as(logits)\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "        \n",
    "# Class for the ANN model (binary classification)\n",
    "class DeepBinary(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=4, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.LazyLinear(hidden_dim))\n",
    "        layers.append(nn.LayerNorm(hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, 1))  # final logit\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Defining NeuralNet class which will be necessary for use with skorch and skopt\n",
    "class NeuralNetBinaryClassifier(NeuralNetClassifier):\n",
    "    def predict_proba(self, X):\n",
    "        logits = self.forward(X).detach().cpu().numpy()\n",
    "        probs = expit(logits)\n",
    "        return np.hstack((1 - probs, probs))\n",
    "\n",
    "############################################\n",
    "### Defining Deep Learning Skorch Set up ###\n",
    "############################################\n",
    "\n",
    "# Defining the base ANN model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = NeuralNetBinaryClassifier(\n",
    "    module = DeepBinary,\n",
    "    criterion = FocalLoss,\n",
    "    criterion__alpha = 0.25,\n",
    "    criterion__gamma = 2.0,\n",
    "    max_epochs = 500,\n",
    "    lr = 1e-3,\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer__weight_decay=1e-4,\n",
    "    batch_size = 128,\n",
    "    device = device,\n",
    "    verbose = 0,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor='valid_loss',\n",
    "            threshold=0.01,\n",
    "            patience=100,\n",
    "            lower_is_better=True,\n",
    "            load_best=False\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a9062",
   "metadata": {},
   "source": [
    "### Defining the hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aa57f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the ANN search space\n",
    "deep_search_space = {\n",
    "    \"lr\": Real(0.000001, 0.01, prior=\"log-uniform\"),\n",
    "    \"module__hidden_dim\": Integer(4, 256),\n",
    "    \"module__num_layers\": Integer(1, 4),\n",
    "    \"module__dropout_rate\": Real(0.0, 0.5),\n",
    "    \"criterion__alpha\": Real(0.0, 0.5),\n",
    "    \"criterion__gamma\": Real(1.0, 7.0)\n",
    "}\n",
    "\n",
    "############################################\n",
    "### Defining non ANN model search spaces ###\n",
    "############################################\n",
    "\n",
    "search_spaces = {\n",
    "    \"SVM\": (\n",
    "        SVC(probability=True, class_weight=\"balanced\", kernel = 'rbf', gamma='scale', random_state=42),\n",
    "        {\n",
    "            \"C\": Real(0.001, 1.0, prior=\"log-uniform\")\n",
    "        }\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": Integer(50, 500),\n",
    "            \"max_depth\": Integer(2, 20),\n",
    "            \"min_samples_split\": Integer(2, 20),\n",
    "            \"min_samples_leaf\": Integer(1, 10),\n",
    "        }\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(eval_metric=\"logloss\", random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": Integer(50, 500),\n",
    "            \"max_depth\": Integer(2, 20),\n",
    "            \"learning_rate\": Real(0.001, 0.1, prior=\"log-uniform\"),\n",
    "            \"subsample\": Real(0.5, 1.0),\n",
    "            \"colsample_bytree\": Real(0.5, 1.0),\n",
    "            \"scale_pos_weight\": Real(1.0, 10.0)\n",
    "        }\n",
    "    ),\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", solver=\"lbfgs\"),\n",
    "        {\n",
    "            \"C\": Real(0.001, 1.0, prior=\"log-uniform\")\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e6fcf",
   "metadata": {},
   "source": [
    "### Setting up utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "185b4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "### Defining Utility Functions ###\n",
    "##################################\n",
    "\n",
    "# Alternative Function for tuning on log-rank z\n",
    "def tune_threshold_by_logrank(\n",
    "    probs_train: np.ndarray,\n",
    "    time_train: np.ndarray,\n",
    "    event_train: np.ndarray,\n",
    ") -> tuple[float, float]:\n",
    "    \n",
    "    probs_train = np.asarray(probs_train, float).ravel()\n",
    "    time_train  = np.asarray(time_train,  float).ravel()\n",
    "    event_train = np.asarray(event_train, bool).ravel()\n",
    "\n",
    "    # Candidate thresholds from fixed quantiles\n",
    "    qs = np.linspace(0.3, 0.7, 41)\n",
    "    cands = np.unique(np.quantile(probs_train, qs))\n",
    "    best_thr  = float(np.median(probs_train))\n",
    "    best_stat = -np.inf\n",
    "    found     = False\n",
    "\n",
    "    for thr in cands:\n",
    "        hi = probs_train >= thr\n",
    "        lo = ~hi\n",
    "        if hi.sum() == 0 or lo.sum() == 0:\n",
    "            continue\n",
    "        try:\n",
    "            lr = logrank_test(\n",
    "                time_train[hi], time_train[lo],\n",
    "                event_observed_A=event_train[hi],\n",
    "                event_observed_B=event_train[lo],\n",
    "            )\n",
    "            chi2 = float(lr.test_statistic)\n",
    "            if np.isfinite(chi2) and chi2 > best_stat:\n",
    "                best_stat = chi2\n",
    "                best_thr  = float(thr)\n",
    "                found     = True\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if not found:\n",
    "        # fallback: median threshold, 0 separation\n",
    "        return float(np.median(probs_train)), 0.0\n",
    "\n",
    "    return best_thr, best_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cad19c",
   "metadata": {},
   "source": [
    "### Setting up the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73c02c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCGA Columns == CGGA columns: True\n"
     ]
    }
   ],
   "source": [
    "# Downloading TCGA Data (Training)\n",
    "TCGA_URL = \"./iMAT_integrated_data/TCGA_iMAT_integrated_df_21.csv\"\n",
    "TCGA = pd.read_csv(TCGA_URL).drop(columns=[\"sample\"])\n",
    "TCGA = TCGA.dropna()\n",
    "\n",
    "# Downloading CGGA Data (Validation)\n",
    "CGGA_URL = \"./CGGA_Data/CGGA_Tidied_Integrated.csv\"\n",
    "CGGA = pd.read_csv(CGGA_URL).drop(columns=[\"CGGA_ID\"])\n",
    "CGGA = CGGA.dropna()\n",
    "\n",
    "# Checking Columns Match\n",
    "print(f\"TCGA Columns == CGGA columns: {TCGA.columns.to_list()==CGGA.columns.to_list()}\")\n",
    "\n",
    "# Combining the dataframes\n",
    "df_LGG = pd.concat([TCGA,CGGA],axis=0)\n",
    "\n",
    "# Setting up X and y\n",
    "LGG_OS = df_LGG[[\"OS\", \"OS.time\"]]\n",
    "le = LabelEncoder().fit(df_LGG[\"OS\"])\n",
    "\n",
    "X_LGG = df_LGG.drop(columns = [\"OS\", \"OS.time\"])\n",
    "y_LGG = le.transform(df_LGG[\"OS\"])\n",
    "\n",
    "# Setting up survival data\n",
    "LGG_event = LGG_OS['OS'].values.astype(bool)\n",
    "LGG_time  = LGG_OS['OS.time'].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dccf6621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3475026567481403"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_LGG)/len(y_LGG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d0db3",
   "metadata": {},
   "source": [
    "### Defining the Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5372417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(random_state=42,inner_folds=3,inner_iterations=25,ANN_iterations=25,savepath=None):\n",
    "\n",
    "    # Stetting variables\n",
    "    RANDOM_STATE = random_state\n",
    "    INNER_FOLDS = inner_folds\n",
    "    N_ITER_INNER = inner_iterations\n",
    "    N_ITER_ANN = ANN_iterations\n",
    "    N_JOBS = -1\n",
    "\n",
    "    # Preparing storage dictionaries\n",
    "    models_info = {name: {'estimator': m, 'space': s} for name, (m, s) in search_spaces.items()}\n",
    "    models_info['ANN'] = {'estimator': net, 'space': deep_search_space}\n",
    "    probs_train_store = {}\n",
    "    thr_values = {}\n",
    "\n",
    "    for model_name, info in models_info.items():\n",
    "        with open(\"./Complete_Model/training_log.txt\", \"a\") as file:\n",
    "            print(f'\\nTuning and Fitting: {model_name}',file=file)\n",
    "        base = clone(info['estimator'])\n",
    "        space = info['space']\n",
    "\n",
    "        # Defining the pipeline for the model\n",
    "        pipe = Pipeline([\n",
    "            ('low_var', VarianceThreshold()),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('smote', SMOTE(random_state=RANDOM_STATE,sampling_strategy='auto')),\n",
    "            ('clf', base)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # prefix search space\n",
    "        space_prefixed = {f'clf__{k}': v for k, v in space.items()}\n",
    "\n",
    "\n",
    "        #Selecting iterations\n",
    "        n_iter = N_ITER_ANN if model_name == 'ANN' else N_ITER_INNER\n",
    "        n_jobs = 1 if model_name == 'ANN' else N_JOBS\n",
    "\n",
    "        opt=BayesSearchCV(\n",
    "            estimator=pipe,\n",
    "            search_spaces=space_prefixed,\n",
    "            n_iter=n_iter,\n",
    "            scoring='average_precision',\n",
    "            cv=StratifiedKFold(n_splits=INNER_FOLDS, shuffle=True, random_state=RANDOM_STATE),\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=n_jobs,\n",
    "            refit=True,\n",
    "        )\n",
    "\n",
    "        # Fitting\n",
    "        fit_X = X_LGG.astype(np.float32) if model_name == 'ANN' else X_LGG\n",
    "        opt.fit(fit_X, y_LGG)\n",
    "\n",
    "        # Plotting Loss Curves\n",
    "        if model_name == \"ANN\":\n",
    "            best = opt.best_estimator_\n",
    "            models_info[model_name][\"trained_estimator\"] = best\n",
    "            # skorch Net is inside the pipeline as step 'clf'\n",
    "            net_trained = best.named_steps['clf']\n",
    "\n",
    "            # Access the training history\n",
    "            history = net_trained.history_\n",
    "\n",
    "            # Extract values\n",
    "            train_losses = history[:, 'train_loss']\n",
    "            valid_losses = history[:, 'valid_loss']\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.plot(train_losses, label=\"Train Loss\")\n",
    "            if valid_losses is not None:\n",
    "                plt.plot(valid_losses, label=\"Valid Loss\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(f\"ANN Loss Curve\")\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.savefig(savepath)\n",
    "            plt.close()\n",
    "\n",
    "        with open(\"./Complete_Model/training_log.txt\", \"a\") as file:\n",
    "            print(f\"Best params for {model_name}: {opt.best_params_}\",file=file)\n",
    "\n",
    "        # In this case fit base estimator inside the pipeline without search\n",
    "        best = opt.best_estimator_\n",
    "        models_info[model_name][\"trained_estimator\"] = best\n",
    "        \n",
    "        # Tuning for best threshold:\n",
    "        train_preds_input = fit_X.astype(np.float32) if model_name == 'ANN' else fit_X\n",
    "        probs_train = best.predict_proba(train_preds_input)[:, 1].ravel()\n",
    "\n",
    "        # Store training probs\n",
    "        probs_train_store[model_name] = probs_train\n",
    "\n",
    "        # Tuning threshold on training data\n",
    "        thr, thr_best = tune_threshold_by_logrank(probs_train=probs_train, time_train=LGG_time,event_train=LGG_event)\n",
    "        thr_values[model_name] = thr\n",
    "        with open(\"./Complete_Model/training_log.txt\", \"a\") as file:\n",
    "            print(f\"Tuned threshold for {model_name}: {thr:.2f} (Log-Rank Chi2={thr_best:.3f})\",file=file)\n",
    "\n",
    "    # Computing Ensemble predictions for this fold against the train set for threshold tuning\n",
    "    model_list = [\"SVM\",\"RandomForest\",\"XGBoost\",\"LogisticRegression\",\"ANN\"]\n",
    "    L_train = np.vstack([probs_train_store[m]for m in model_list]).T\n",
    "    std_scaler = StandardScaler().fit(L_train)\n",
    "    Z_train = std_scaler.transform(L_train)\n",
    "\n",
    "    probs_train_df = pd.DataFrame(Z_train, columns=model_list)\n",
    "    probs_train_df[\"OS\"] = LGG_event\n",
    "    probs_train_df[\"OS.time\"] = LGG_time\n",
    "\n",
    "    cph = CoxPHFitter(penalizer=0.05, l1_ratio=0.0)\n",
    "    cph.fit(probs_train_df, duration_col=\"OS.time\", event_col=\"OS\", robust=True)\n",
    "\n",
    "    beta_vec = cph.params_[model_list].values\n",
    "    eta_train = Z_train @ beta_vec               \n",
    "\n",
    "    # Threshold tuning on TRAIN\n",
    "    thr_ens, thr_ens_best = tune_threshold_by_logrank(eta_train, LGG_time, LGG_event)\n",
    "    thr_values[\"Ensemble\"] = thr_ens\n",
    "    probs_train_store['Ensemble'] = eta_train\n",
    "    with open(\"./Complete_Model/training_log.txt\", \"a\") as file:\n",
    "        print(f\"Tuned threshold for Ensemble: {thr_ens:.2f} (Log-Rank Chi2={thr_ens_best:.3f})\",file=file)\n",
    "\n",
    "\n",
    "    save_root = Path(\"./Complete_Model\")\n",
    "    (save_root / \"models\").mkdir(parents=True, exist_ok=True)\n",
    "    (save_root / \"thresholds\").mkdir(exist_ok=True)\n",
    "    (save_root / \"ensemble\").mkdir(exist_ok=True)\n",
    "    \n",
    "    #############################\n",
    "    # 1. Save classical models\n",
    "    #############################\n",
    "    for model_name in model_list:  # SVM, RF, XGB, LR, ANN\n",
    "        if model_name != \"ANN\":\n",
    "            best_model = models_info[model_name][\"trained_estimator\"]\n",
    "            joblib.dump(best_model, save_root / f\"models/{model_name}.pkl\")\n",
    "\n",
    "    #############################\n",
    "    # 2. Save ANN pipeline (.pkl)\n",
    "    #############################\n",
    "    ann_pipeline = models_info[\"ANN\"][\"trained_estimator\"]\n",
    "    joblib.dump(ann_pipeline, save_root / \"models/ANN_pipeline.pkl\")\n",
    "\n",
    "    #############################\n",
    "    # 3. Save ANN weights (.pt)\n",
    "    #############################\n",
    "    ann_clf = ann_pipeline.named_steps[\"clf\"]\n",
    "    ann_clf.save_params(f_params=str(save_root / \"models/ANN_params.pt\"))\n",
    "\n",
    "    #############################\n",
    "    # 4. Create HF-ready ANN config\n",
    "    #############################\n",
    "    # Extract ANN module (PyTorch model)\n",
    "    # good â€” use a new name that doesn't clash\n",
    "    ann_module = ann_clf.module_\n",
    "\n",
    "\n",
    "    ann_config = {\n",
    "        \"model_name\": \"DeepBinary\",\n",
    "        \"architecture\": {\n",
    "            \"hidden_dim\": getattr(ann_module, \"hidden_dim\", None),\n",
    "            \"num_layers\": getattr(ann_module, \"num_layers\", None),\n",
    "            \"dropout_rate\": getattr(ann_module, \"dropout_rate\", None),\n",
    "            \"output_dim\": 2,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"optimizer\": \"Adam\",\n",
    "            \"learning_rate\": ann_clf.lr,\n",
    "            \"batch_size\": ann_clf.batch_size,\n",
    "            \"max_epochs\": ann_clf.max_epochs,\n",
    "            \"criterion\": str(ann_clf.criterion),\n",
    "        },\n",
    "        \"random_state\": random_state,\n",
    "        \"versions\": {\n",
    "            \"python\": sys.version,\n",
    "            \"pytorch\": torch.__version__,\n",
    "            \"skorch\": ann_clf.__class__.__module__,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(save_root / \"models/ANN_config.json\", \"w\") as f:\n",
    "        json.dump(ann_config, f, indent=4)\n",
    "\n",
    "    #############################\n",
    "    # 5. Save per-model thresholds\n",
    "    #############################\n",
    "    json.dump(\n",
    "        {m: float(thr_values.get(m)) for m in model_list},\n",
    "        open(save_root / \"thresholds/per_model_thresholds.json\", \"w\"),\n",
    "        indent=4\n",
    "    )\n",
    "\n",
    "    #############################\n",
    "    # 6. Save ensemble components\n",
    "    #############################\n",
    "    joblib.dump(std_scaler, save_root / \"ensemble/scaler.pkl\")\n",
    "    np.save(save_root / \"ensemble/beta_vec.npy\", beta_vec)\n",
    "\n",
    "    json.dump(\n",
    "        {\"ensemble_threshold\": float(thr_ens)},\n",
    "        open(save_root / \"ensemble/ens_threshold.json\", \"w\"),\n",
    "        indent=4\n",
    "    )\n",
    "\n",
    "    #############################\n",
    "    # 7. Save global metadata\n",
    "    #############################\n",
    "    metadata = {\n",
    "        \"random_state\": random_state,\n",
    "        \"inner_folds\": inner_folds,\n",
    "        \"inner_iterations\": inner_iterations,\n",
    "        \"ann_iterations\": ANN_iterations,\n",
    "        \"model_list\": model_list,\n",
    "        \"python_version\": sys.version,\n",
    "    }\n",
    "    json.dump(metadata, open(save_root / \"metadata.json\", \"w\"), indent=4)\n",
    "\n",
    "    print(\"\\n All models and config files successfully saved\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbcdb3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All models and config files successfully saved\n"
     ]
    }
   ],
   "source": [
    "train_evaluate_model(random_state=0,inner_folds=5,inner_iterations=50,ANN_iterations=50, savepath=f\"./Complete_Model/ANN_Figures.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
