{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d76df3",
   "metadata": {},
   "source": [
    "# Final TCGA Model Training/Hyperparameter Opotimisation and validation against CGGA data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e465b",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f269952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "from scipy.special import expit, logit\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, average_precision_score,\n",
    "                             roc_curve, precision_recall_curve, balanced_accuracy_score)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sksurv.metrics import cumulative_dynamic_auc\n",
    "from sksurv.util import Surv\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EarlyStopping, Callback\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from dcurves import dca, plot_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b98c7",
   "metadata": {},
   "source": [
    "### Defining ANN Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### Defining Helper Classes for ANN ###\n",
    "#######################################\n",
    "\n",
    "# Focal Loss class for enabling training focused on the difficult to predict class\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=5, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        targets = targets.view(-1,1).type_as(logits)\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "        \n",
    "# Class for the ANN model (binary classification)\n",
    "class DeepBinary(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=4, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.LazyLinear(hidden_dim))\n",
    "        layers.append(nn.LayerNorm(hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, 1))  # final logit\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Defining NeuralNet class which will be necessary for use with skorch and skopt\n",
    "class NeuralNetBinaryClassifier(NeuralNetClassifier):\n",
    "    def predict_proba(self, X):\n",
    "        logits = self.forward(X).detach().cpu().numpy()\n",
    "        probs = expit(logits)\n",
    "        return np.hstack((1 - probs, probs))\n",
    "\n",
    "############################################\n",
    "### Defining Deep Learning Skorch Set up ###\n",
    "############################################\n",
    "\n",
    "# Defining the base ANN model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = NeuralNetBinaryClassifier(\n",
    "    module = DeepBinary,\n",
    "    criterion = FocalLoss,\n",
    "    criterion__alpha = 0.25,\n",
    "    criterion__gamma = 2.0,\n",
    "    max_epochs = 500,\n",
    "    lr = 1e-3,\n",
    "    optimizer = torch.optim.Adam,\n",
    "    optimizer__weight_decay=1e-4,\n",
    "    batch_size = 128,\n",
    "    device = device,\n",
    "    verbose = 0,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor='valid_loss',\n",
    "            threshold=0.01,\n",
    "            patience=25,\n",
    "            lower_is_better=True,\n",
    "            load_best=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b4b33",
   "metadata": {},
   "source": [
    "### Defining Hyperparameter Search Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19226d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the ANN search space\n",
    "deep_search_space = {\n",
    "    \"lr\": Real(0.000001, 0.01, prior=\"log-uniform\"),\n",
    "    \"module__hidden_dim\": Integer(4, 256),\n",
    "    \"module__num_layers\": Integer(2, 8),\n",
    "    \"module__dropout_rate\": Real(0.0, 0.5),\n",
    "    \"criterion__alpha\": Real(0.0, 0.5),\n",
    "    \"criterion__gamma\": Real(1.0, 7.0)\n",
    "}\n",
    "\n",
    "############################################\n",
    "### Defining non ANN model search spaces ###\n",
    "############################################\n",
    "\n",
    "search_spaces = {\n",
    "    \"SVM\": (\n",
    "        SVC(probability=True, class_weight=\"balanced\", kernel = 'rbf', gamma='scale', random_state=42),\n",
    "        {\n",
    "            \"C\": Real(0.001, 1.0, prior=\"log-uniform\")\n",
    "        }\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(class_weight=\"balanced\", random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": Integer(50, 500),\n",
    "            \"max_depth\": Integer(2, 20),\n",
    "            \"min_samples_split\": Integer(2, 20),\n",
    "            \"min_samples_leaf\": Integer(1, 10),\n",
    "        }\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(eval_metric=\"logloss\", random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": Integer(50, 500),\n",
    "            \"max_depth\": Integer(2, 20),\n",
    "            \"learning_rate\": Real(0.001, 0.1, prior=\"log-uniform\"),\n",
    "            \"subsample\": Real(0.5, 1.0),\n",
    "            \"colsample_bytree\": Real(0.5, 1.0),\n",
    "            \"scale_pos_weight\": Real(1.0, 10.0)\n",
    "        }\n",
    "    ),\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression(max_iter=5000, class_weight=\"balanced\", solver=\"lbfgs\"),\n",
    "        {\n",
    "            \"C\": Real(0.001, 1.0, prior=\"log-uniform\")\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226d076",
   "metadata": {},
   "source": [
    "### Defining utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4623e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "### Defining Utility Functions ###\n",
    "##################################\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(y_true, probs, threshold=0.5):\n",
    "    y_pred = (probs >= threshold).astype(int)\n",
    "    return {'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_true, probs),\n",
    "            'pr_auc': average_precision_score(y_true, probs)}\n",
    "\n",
    "# Function for tuning thresholds with Binary F1\n",
    "def tune_threshold(probs, y_true):\n",
    "    thresholds = np.linspace(0.25, 0.75, 101)\n",
    "    best_thr, best_score = 0.5, -1.0\n",
    "    for t in thresholds:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        score = f1_score(y_true, preds,average='binary')\n",
    "        if score > best_score:\n",
    "            best_score, best_thr = score, t\n",
    "    return best_thr, best_score\n",
    "\n",
    "# Alternative Function for tuning on log-rank z\n",
    "def tune_threshold_by_logrank(\n",
    "    probs_train: np.ndarray,\n",
    "    time_train: np.ndarray,\n",
    "    event_train: np.ndarray,\n",
    ") -> tuple[float, float]:\n",
    "    \n",
    "    probs_train = np.asarray(probs_train, float).ravel()\n",
    "    time_train  = np.asarray(time_train,  float).ravel()\n",
    "    event_train = np.asarray(event_train, bool).ravel()\n",
    "\n",
    "    # Candidate thresholds from fixed quantiles\n",
    "    qs = np.linspace(0.10, 0.70, 61)\n",
    "    cands = np.unique(np.quantile(probs_train, qs))\n",
    "    print(cands)\n",
    "    best_thr  = float(np.median(probs_train))\n",
    "    best_stat = -np.inf\n",
    "    found     = False\n",
    "\n",
    "    for thr in cands:\n",
    "        hi = probs_train >= thr\n",
    "        lo = ~hi\n",
    "        if hi.sum() == 0 or lo.sum() == 0:\n",
    "            continue\n",
    "        try:\n",
    "            lr = logrank_test(\n",
    "                time_train[hi], time_train[lo],\n",
    "                event_observed_A=event_train[hi],\n",
    "                event_observed_B=event_train[lo],\n",
    "            )\n",
    "            chi2 = float(lr.test_statistic)\n",
    "            if np.isfinite(chi2) and chi2 > best_stat:\n",
    "                best_stat = chi2\n",
    "                best_thr  = float(thr)\n",
    "                found     = True\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if not found:\n",
    "        # fallback: median threshold, 0 separation\n",
    "        return float(np.median(probs_train)), 0.0\n",
    "\n",
    "    return best_thr, best_stat\n",
    "\n",
    "# aggregaring folds for cumulative auc plotting\n",
    "def _aggregate_cd_auc(per_fold_cd: Dict[str, List[Tuple[np.ndarray, np.ndarray]]],\n",
    "                      n_times_common: int = 75) -> Dict[str, dict]:\n",
    "    \"\"\"Interpolate per-fold cumulative_dynamic_auc onto a common grid and\n",
    "    return mean/std envelopes per model.\n",
    "\n",
    "    per_fold_cd[model] = list of (times_k, auc_k) for each outer fold.\n",
    "    \"\"\"\n",
    "    out: Dict[str, dict] = {}\n",
    "\n",
    "    for model, fold_list in per_fold_cd.items():\n",
    "        if not fold_list:\n",
    "            continue\n",
    "\n",
    "        # Validate and normalize each (times, auc) pair\n",
    "        grids: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "        for times_k, auc_k in fold_list:\n",
    "            t = np.asarray(times_k, dtype=float).ravel()\n",
    "            a = np.asarray(auc_k, dtype=float).ravel()\n",
    "            if t.ndim != 1 or a.ndim != 1 or len(t) == 0 or len(t) != len(a):\n",
    "                # Skip malformed entries\n",
    "                continue\n",
    "            grids.append((t, a))\n",
    "\n",
    "        if not grids:\n",
    "            continue\n",
    "\n",
    "        # Build a common time grid that lies inside *all* fold grids\n",
    "        lows = [t[0] for t, _ in grids]\n",
    "        highs = [t[-1] for t, _ in grids]\n",
    "        t_lo = max(lows)\n",
    "        t_hi = min(highs)\n",
    "\n",
    "        if not np.isfinite(t_lo) or not np.isfinite(t_hi) or t_hi <= t_lo:\n",
    "            # Fallback to median overlap\n",
    "            t_lo = float(np.median(lows))\n",
    "            t_hi = float(np.median(highs))\n",
    "        if t_hi <= t_lo:\n",
    "            # Give up on this model if no overlap exists\n",
    "            continue\n",
    "\n",
    "        common_times = np.linspace(t_lo, t_hi, n_times_common)\n",
    "\n",
    "        # Accumulate interpolated rows in a *list*, then stack once\n",
    "        auc_rows: List[np.ndarray] = []\n",
    "        for t, a in grids:\n",
    "            auc_rows.append(np.interp(common_times, t, a))\n",
    "\n",
    "        if not auc_rows:\n",
    "            continue\n",
    "        auc_mat = np.vstack(auc_rows)\n",
    "\n",
    "        out[model] = {\n",
    "            \"times_days\": common_times,\n",
    "            \"times_years\": common_times / 365.0,\n",
    "            \"mean\": auc_mat.mean(axis=0),\n",
    "            \"std\": auc_mat.std(axis=0),\n",
    "        }\n",
    "\n",
    "    return out\n",
    "\n",
    "# Preparing safe time grids for cumulative auc\n",
    "def make_time_grid_quantile(y_train_struct, y_test_struct, max_points: int = 40,\n",
    "                            lo_q: float = 0.10, hi_q: float = 0.90) -> np.ndarray:\n",
    "    ev_times = np.asarray(y_test_struct[\"time\"][y_test_struct[\"event\"]], dtype=float)\n",
    "    if ev_times.size == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # Trim extremes on the test events and cap by train max time (required by sksurv)\n",
    "    lo = np.quantile(ev_times, lo_q)\n",
    "    hi = np.quantile(ev_times, hi_q)\n",
    "    hi = min(hi, float(np.max(y_train_struct[\"time\"])) - 1e-8)\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        return np.array([])\n",
    "\n",
    "    # Use quantiles to place points where data exist\n",
    "    uniq = np.unique(ev_times[(ev_times >= lo) & (ev_times <= hi)])\n",
    "    if uniq.size == 0:\n",
    "        return np.array([])\n",
    "    k = int(min(max_points, max(10, uniq.size)))  # 10â€“40 points typical\n",
    "    qs = np.linspace(lo_q, hi_q, k)\n",
    "    grid = np.quantile(ev_times, qs)\n",
    "    return np.unique(grid)\n",
    "\n",
    "# Filtering time grids for cumulative auc to avoid nans\n",
    "def filter_auc_grid_by_comparable_pairs(y_test_struct, time_grid: np.ndarray) -> np.ndarray:\n",
    "    if time_grid.size == 0:\n",
    "        return time_grid\n",
    "    times = np.asarray(y_test_struct[\"time\"], dtype=float)\n",
    "    events = np.asarray(y_test_struct[\"event\"], dtype=bool)\n",
    "\n",
    "    t_col = time_grid[:, None]    # (T,1)\n",
    "    times_row = times[None, :]    # (1,N)\n",
    "    events_row = events[None, :]  # (1,N)\n",
    "\n",
    "    cases = ((events_row) & (times_row <= t_col)).sum(axis=1)  # events by t\n",
    "    at_risk = (times_row >= t_col).sum(axis=1)                 # still at risk at t\n",
    "    controls = at_risk - cases\n",
    "\n",
    "    mask = (cases > 0) & (controls > 0)\n",
    "    return time_grid[mask]\n",
    "\n",
    "# Safely using sksurv\n",
    "def compute_cd_auc_robust(y_train_struct, y_test_struct, risk_scores: np.ndarray,\n",
    "                          base_grid: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return (times, auc_t) with invalid times removed. If <2 valid points, return ([], []).\"\"\"\n",
    "    t_eval = filter_auc_grid_by_comparable_pairs(y_test_struct, base_grid)\n",
    "    if t_eval.size < 2:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    auc_t, _ = cumulative_dynamic_auc(y_train_struct, y_test_struct, risk_scores, t_eval)\n",
    "    auc_t = np.asarray(auc_t, dtype=float)\n",
    "\n",
    "    # Drop any remaining non-finite values and align times\n",
    "    m = np.isfinite(auc_t)\n",
    "    t_eval, auc_t = t_eval[m], auc_t[m]\n",
    "    if t_eval.size < 2:\n",
    "        return np.array([]), np.array([])\n",
    "    return t_eval, auc_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f8264",
   "metadata": {},
   "source": [
    "### Defining plotting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a025bea",
   "metadata": {},
   "source": [
    "### Defining training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acec5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(random_state=42,inner_folds=3,inner_iterations=25,ANN_iterations=25):\n",
    "\n",
    "    # Stetting variables\n",
    "    RANDOM_STATE = random_state\n",
    "    INNER_FOLDS = inner_folds\n",
    "    N_ITER_INNER = inner_iterations\n",
    "    N_ITER_ANN = ANN_iterations\n",
    "    N_JOBS = -1\n",
    "\n",
    "    # Downloading TCGA Data (Training)\n",
    "    TCGA_URL = \"https://raw.githubusercontent.com/JackWJW/LGG_Prognosis_Prediction/main/Tidied_Datasets/tidied_integrated_df_9.csv\"\n",
    "    TCGA = pd.read_csv(TCGA_URL).drop(columns=[\"Unnamed: 0\"])\n",
    "    TCGA = TCGA.dropna()\n",
    "    TCGA.columns = TCGA.columns.str.rstrip('_9')\n",
    "    TCGA = TCGA.rename(columns={'FUT':'FUT9'})\n",
    "    TCGA_dss = TCGA[[\"DSS\", \"DSS.time\"]]\n",
    "\n",
    "    le = LabelEncoder().fit(TCGA[\"DSS\"])\n",
    "\n",
    "    X_TCGA = TCGA.drop(columns = [\"Srv\", \"DSS\", \"DSS.time\"])\n",
    "    y_TCGA = le.transform(TCGA[\"DSS\"])\n",
    "\n",
    "    # Downloading CGGA Data (Validation)\n",
    "    CGGA_URL = \"https://raw.githubusercontent.com/JackWJW/LGG_Prognosis_Prediction/main/CGGA_Data/CGGA_Tidied_Integrated.csv\"\n",
    "    CGGA = pd.read_csv(CGGA_URL).drop(columns=[\"CGGA_ID\"])\n",
    "    CGGA = CGGA.dropna()\n",
    "    CGGA_dss = CGGA[[\"DSS\", \"DSS.time\"]]\n",
    "\n",
    "    X_CGGA = CGGA.drop(columns = [\"Srv\", \"DSS\", \"DSS.time\"])\n",
    "    y_CGGA = le.transform(CGGA[\"DSS\"])\n",
    "\n",
    "    # Preparing storage dictionaries\n",
    "    models_info = {name: {'estimator': m, 'space': s} for name, (m, s) in search_spaces.items()}\n",
    "    models_info['ANN'] = {'estimator': net, 'space': deep_search_space}\n",
    "\n",
    "    # Setting up event and time datasets\n",
    "    TCGA_event = TCGA_dss['DSS'].values.astype(bool)\n",
    "    TCGA_time  = TCGA_dss['DSS.time'].values.astype(float)\n",
    "\n",
    "    CGGA_event  = CGGA_dss['DSS'].values.astype(bool)\n",
    "    CGGA_time   = CGGA_dss['DSS.time'].values.astype(float)\n",
    "\n",
    "    TCGA_Surv = Surv.from_arrays(event=TCGA_event, time=TCGA_time)\n",
    "    CGGA_Surv  = Surv.from_arrays(event=CGGA_event, time=CGGA_time)\n",
    "\n",
    "    fold_time_grid = make_time_grid_quantile(TCGA_Surv, CGGA_Surv, max_points=40)\n",
    "\n",
    "    # Setting up results dataframes\n",
    "    probs_train_store = {}\n",
    "    probs_results = {'y_true':y_CGGA}\n",
    "    class_results = {'y_true':y_CGGA}\n",
    "    metrics_results = {}\n",
    "    roc_results = {}\n",
    "    pr_results = {}\n",
    "    cd_auc_results = {}\n",
    "\n",
    "    for model_name, info in models_info.items():\n",
    "        print(f'\\nTuning and Fitting: {model_name}')\n",
    "        base = clone(info['estimator'])\n",
    "        space = info['space']\n",
    "\n",
    "        # Defining the pipeline for the model\n",
    "        if model_name == 'ANN':\n",
    "            pipe = Pipeline([\n",
    "                ('low_var', VarianceThreshold(threshold=0.01)),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('smote', SMOTE(random_state=RANDOM_STATE,sampling_strategy=0.5)),\n",
    "                ('clf', base)\n",
    "            ])\n",
    "\n",
    "        else:\n",
    "            pipe = Pipeline([\n",
    "                ('low_var', VarianceThreshold(threshold=0.01)),\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('smote', SMOTE(random_state=RANDOM_STATE,sampling_strategy=0.5)),\n",
    "                ('clf', base)\n",
    "            ])\n",
    "\n",
    "        # prefix search space\n",
    "        space_prefixed = {f'clf__{k}': v for k, v in space.items()}\n",
    "\n",
    "        #Selecting iterations\n",
    "        n_iter = N_ITER_ANN if model_name == 'ANN' else N_ITER_INNER\n",
    "        n_jobs = 1 if model_name == 'ANN' else N_JOBS\n",
    "\n",
    "        opt=BayesSearchCV(\n",
    "            estimator=pipe,\n",
    "            search_spaces=space_prefixed,\n",
    "            n_iter=n_iter,\n",
    "            scoring='average_precision',\n",
    "            cv=INNER_FOLDS,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=n_jobs,\n",
    "            refit=True\n",
    "        )\n",
    "\n",
    "        # Fitting\n",
    "        fit_X = X_TCGA.astype(np.float32) if model_name == 'ANN' else X_TCGA\n",
    "        opt.fit(fit_X, y_TCGA)\n",
    "\n",
    "        # Plotting Loss Curves\n",
    "        if model_name == \"ANN\":\n",
    "            best = opt.best_estimator_\n",
    "            \n",
    "            # skorch Net is inside the pipeline as step 'clf'\n",
    "            net_trained = best.named_steps['clf']\n",
    "\n",
    "            # Access the training history\n",
    "            history = net_trained.history_\n",
    "\n",
    "            # Extract values\n",
    "            train_losses = history[:, 'train_loss']\n",
    "            valid_losses = history[:, 'valid_loss']\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.plot(train_losses, label=\"Train Loss\")\n",
    "            if valid_losses is not None:\n",
    "                plt.plot(valid_losses, label=\"Valid Loss\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(f\"ANN Loss Curve\")\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        print(f\"Best params for {model_name}: {opt.best_params_}\")\n",
    "\n",
    "        # In this case fit base estimator inside the pipeline without search\n",
    "        pipe.set_params(**{})\n",
    "        pipe.fit(fit_X, y_TCGA)\n",
    "        best = opt.best_estimator_\n",
    "\n",
    "        \n",
    "        # Tuning for best threshold:\n",
    "        train_preds_input = fit_X.astype(np.float32) if model_name == 'ANN' else fit_X\n",
    "        probs_train = best.predict_proba(train_preds_input)[:, 1].ravel()\n",
    "\n",
    "        # Store training probs\n",
    "        probs_train_store[model_name] = probs_train\n",
    "\n",
    "        # Tuning threshold on training data\n",
    "        thr, thr_best = tune_threshold_by_logrank(probs_train=probs_train, time_train=TCGA_time,event_train=TCGA_event)\n",
    "        print(f\"Tuned threshold for {model_name}: {thr:.2f} (Log-Rank Chi2={thr_best:.3f})\")\n",
    "\n",
    "        # Predict proba on the test set\n",
    "        test_X = X_CGGA.astype(np.float32) if model_name == 'ANN' else X_CGGA\n",
    "        probs = best.predict_proba(test_X)[:, 1].ravel()\n",
    "\n",
    "        # Preparation for cumulative dynamic plotting\n",
    "        t_eval, auc_vec = compute_cd_auc_robust(TCGA_Surv, CGGA_Surv, probs, fold_time_grid)\n",
    "        cd_auc_results[model_name] = (t_eval, auc_vec)\n",
    "\n",
    "        # Storing testing probs\n",
    "        probs_results[model_name] = probs\n",
    "        \n",
    "        # Storing out of fold predictions\n",
    "        model_predictions = (probs >= thr).astype(int)\n",
    "        class_results[model_name] = model_predictions\n",
    "\n",
    "        # Compute per-fold metrics and curves\n",
    "        m = compute_metrics(y_CGGA, probs, threshold=thr)\n",
    "        metrics_results[model_name] = m\n",
    "        fpr, tpr, _ = roc_curve(y_CGGA, probs)\n",
    "        prec, rec, _ = precision_recall_curve(y_CGGA, probs)\n",
    "        roc_results[model_name] = (fpr, tpr)\n",
    "        pr_results[model_name] = (rec, prec)\n",
    "\n",
    "        print(f\"{model_name}: AP={m['pr_auc']:.3f}, ROC AUC={m['roc_auc']:.3f}\")\n",
    "    \n",
    "    # Computing Ensemble predictions for this fold against the train set for threshold tuning\n",
    "    L_train_cols = []\n",
    "    model_list = [\"SVM\",\"RandomForest\",\"XGBoost\",\"LogisticRegression\",\"ANN\"]\n",
    "    for m in model_list:\n",
    "        p = np.clip(probs_train_store[m], 1e-6, 1-1e-6)\n",
    "        L_train_cols.append(logit(p))\n",
    "    L_train = np.vstack(L_train_cols).T   # columns ordered as model_list\n",
    "\n",
    "    # Fit scaler on TRAIN, transform TRAIN\n",
    "    std_scaler = StandardScaler().fit(L_train)\n",
    "    Z_train = std_scaler.transform(L_train)\n",
    "\n",
    "    probs_train_df = pd.DataFrame(Z_train, columns=[m for m in model_list])\n",
    "    probs_train_df[\"DSS\"] = TCGA_event\n",
    "    probs_train_df[\"DSS.time\"] = TCGA_time\n",
    "\n",
    "    cph_train_models = CoxPHFitter(penalizer=0.05, l1_ratio=0.0)\n",
    "    cph_train_models.fit(probs_train_df, duration_col = \"DSS.time\", event_col=\"DSS\", robust=True)\n",
    "\n",
    "    models_train_cindex = cph_train_models.concordance_index_\n",
    "    print(f\"\\nTraining Multivariate C-Index={models_train_cindex}\\n\")\n",
    "    \n",
    "    train_summary = cph_train_models.summary.reset_index().rename(columns={'index': 'Gene'})\n",
    "    train_summary_extraction = train_summary.set_index('covariate')\n",
    "\n",
    "    cox_weights_train = {}\n",
    "    weighted_probs_train = []\n",
    "    for m in model_list:\n",
    "        cox_weights_train[m] = train_summary_extraction.loc[m][\"coef\"]\n",
    "        weighted_probs_train.append((probs_train_df[m]*cox_weights_train[m]))\n",
    "    weighted_probs_stack_train = np.vstack(weighted_probs_train)\n",
    "\n",
    "    ensemble_train_probs = np.sum(weighted_probs_stack_train,axis=0)\n",
    "    probs_min_scaler = MinMaxScaler()\n",
    "    ensemble_train_probs = probs_min_scaler.fit_transform(ensemble_train_probs.reshape(-1,1)).flatten()\n",
    "\n",
    "    # Tuning Ensemble threshold on ensemble_train_probs\n",
    "    thr_ens, thr_ens_best = tune_threshold_by_logrank(probs_train=ensemble_train_probs, time_train=TCGA_time,event_train=TCGA_event)\n",
    "    probs_train_store['Ensemble'] = ensemble_train_probs\n",
    "    print(f\"Tuned threshold for Ensemble: {thr_ens:.2f} (Log-Rank Chi2={thr_ens_best:.3f})\")\n",
    "\n",
    "    # Computing Ensemble predictions for this fold against the test set\n",
    "    L_test_cols = []\n",
    "    for m in model_list:\n",
    "        p = np.clip(probs_results[m], 1e-6, 1-1e-6)\n",
    "        L_test_cols.append(logit(p))\n",
    "    L_test = np.vstack(L_test_cols).T\n",
    "    \n",
    "    Z_test = std_scaler.transform(L_test)\n",
    "\n",
    "    probs_test_df = pd.DataFrame(Z_test, columns=[m for m in model_list])\n",
    "    probs_test_df[\"DSS\"] = CGGA_event\n",
    "    probs_test_df[\"DSS.time\"] = CGGA_time\n",
    "\n",
    "    weighted_probs_test = []\n",
    "    for m in model_list:\n",
    "        weighted_probs_test.append((probs_test_df[m]*cox_weights_train[m]))\n",
    "    weighted_probs_stack_test = np.vstack(weighted_probs_test)\n",
    "\n",
    "    ensemble_test_probs = np.sum(weighted_probs_stack_test,axis=0)\n",
    "    ensemble_test_probs = probs_min_scaler.transform(ensemble_test_probs.reshape(-1,1)).flatten()\n",
    "    probs_results['Ensemble'] = ensemble_test_probs\n",
    "\n",
    "    t_eval, auc_vec = compute_cd_auc_robust(TCGA_Surv, CGGA_Surv, ensemble_test_probs, fold_time_grid)\n",
    "    cd_auc_results['Ensemble'] = (t_eval, auc_vec)\n",
    "\n",
    "    #Calculating ensemble predictions\n",
    "    ensemble_preds = (ensemble_test_probs >= thr_ens).astype(int)\n",
    "    class_results['Ensemble'] = ensemble_preds\n",
    "\n",
    "    # Store ensemble metrics and curves\n",
    "    m_ens = compute_metrics(y_CGGA, ensemble_test_probs, threshold=thr_ens)\n",
    "    metrics_results['Ensemble'] = (m_ens)\n",
    "    fpr, tpr, _ = roc_curve(y_CGGA, ensemble_test_probs)\n",
    "    prec, rec, _ = precision_recall_curve(y_CGGA, ensemble_test_probs)\n",
    "    roc_results['Ensemble'] = (fpr, tpr)\n",
    "    pr_results['Ensemble'] = (rec, prec)\n",
    "    \n",
    "    probs_df = pd.DataFrame(probs_results)\n",
    "    class_df = pd.DataFrame(class_results)\n",
    "\n",
    "    model_list = [\"SVM\",\"RandomForest\",\"XGBoost\",\"LogisticRegression\",\"ANN\",\"Ensemble\"]\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        'y_true': y_CGGA,\n",
    "        'DSS.time': CGGA_time,\n",
    "        'DSS': CGGA_event.astype(int)\n",
    "    })\n",
    "\n",
    "    for m in model_list:\n",
    "        summary_df[f'prob_{m}'] = probs_results[m]\n",
    "        summary_df[f'pred_{m}'] = class_results[m]\n",
    "\n",
    "    # metrics_summary = {}\n",
    "    # for m in model_list + ['Ensemble']:\n",
    "    #     vals = metrics_results[m]\n",
    "    #     summary = {k: (np.mean([v[k] for v in vals]), np.std([v[k] for v in vals])) for k in vals[0].keys()}\n",
    "    #     metrics_summary[m] = summary\n",
    "    \n",
    "    curves_summary = {'roc': {}, 'pr': {}}\n",
    "    all_models_for_plot = model_list + ['Ensemble']\n",
    "    for m in all_models_for_plot:\n",
    "        fpr, tpr, _ = roc_curve(probs_df['y_true'], probs_df[m])\n",
    "        prec, rec, _ = precision_recall_curve(probs_df['y_true'], probs_df[m])\n",
    "        curves_summary['roc'][m] = fpr, tpr\n",
    "        curves_summary['pr'][m]  = rec, prec\n",
    "\n",
    "    # # Prepare metrics dictionary\n",
    "    metrics_for_plot = metrics_results\n",
    "\n",
    "    # Preparing survival resutls dictionaries\n",
    "    survival_results = {}\n",
    "    for m in all_models_for_plot:\n",
    "        p = np.clip(probs_df[m].values, 1e-6, 1-1e-6)\n",
    "        logit_p = logit(p)\n",
    "        logit_scaled = zscore(logit_p)\n",
    "\n",
    "        survival_results[m] = [(probs_test_df[['DSS.time', 'DSS']], \n",
    "                                {\"class\": class_df[m].values.astype(int),\n",
    "                                 \"logit_scaled\": logit_scaled})]\n",
    "    \n",
    "    # Preparing results for cumulative auc\n",
    "    # cauc_agg = _aggregate_cd_auc(cd_auc_results, n_times_common=50)\n",
    "\n",
    "    return summary_df, curves_summary, metrics_for_plot, survival_results, y_CGGA, # cauc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df, curves_summary, metrics_for_plot, survival_results, y_CGGA= train_evaluate_model(random_state=0,inner_folds=5,inner_iterations=25,ANN_iterations=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot mean ROC curves\n",
    "def plot_mean_roc(curves_summary):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    for model_name, (fpr, mean_tpr) in curves_summary['roc'].items():\n",
    "        plt.plot(fpr, mean_tpr, label=f'{model_name}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.title(\"CV Pooled ROC-Curves\", fontsize=18)\n",
    "    plt.tick_params(axis=\"both\", labelsize=14)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Function to plot mean PR curves\n",
    "def plot_mean_pr(curves_summary):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    for model_name, (recall, mean_prec) in curves_summary['pr'].items():\n",
    "        plt.plot(recall, mean_prec, label=f'{model_name}')\n",
    "    plt.hlines(y_CGGA.sum()/len(y_CGGA), 0, 1, colors=\"k\", linestyles=\"--\", label=\"Baseline\")\n",
    "    plt.xlabel('Recall', fontsize=16)\n",
    "    plt.ylabel('Precision', fontsize=16)\n",
    "    plt.title(\"CV Pooled PR-Curves\", fontsize=18)\n",
    "    plt.tick_params(axis=\"both\", labelsize=14)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_mean_roc(curves_summary)\n",
    "plot_mean_pr(curves_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_km_curves(survival_results, max_years=5):\n",
    "    risk_labels = {0: \"Low Risk\", 1: \"High Risk\"}\n",
    "    model_list = list(survival_results.keys())\n",
    "    colours = [\"#C190F0\", \"#35AB6A\"]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    logrank_ps = []\n",
    "    iteration = 0\n",
    "    for idx, model_name in enumerate(model_list):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        # Pool all folds\n",
    "        dfs = []\n",
    "        for dss_val, pred_dict in survival_results[model_name]:\n",
    "            temp = dss_val.copy()\n",
    "            temp[\"pred_class\"] = pred_dict[\"class\"]\n",
    "            dfs.append(temp)\n",
    "        pooled_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        high = pooled_df[pooled_df[\"pred_class\"] == 1]\n",
    "        low  = pooled_df[pooled_df[\"pred_class\"] == 0]\n",
    "        lr_res = logrank_test(\n",
    "            high[\"DSS.time\"], low[\"DSS.time\"],\n",
    "            event_observed_A=high[\"DSS\"], event_observed_B=low[\"DSS\"]\n",
    "        )\n",
    "        p_val = lr_res.p_value\n",
    "        logrank_ps.append(p_val)\n",
    "        ax.set_title(f\"{model_name}\\nLog-Rank p = {logrank_ps[iteration]:.2e}\", fontsize=16)\n",
    "        iteration += 1\n",
    "        # KM curves\n",
    "        kmf = KaplanMeierFitter()\n",
    "        for cls in [0, 1]:\n",
    "            mask = pooled_df[\"pred_class\"] == cls\n",
    "            n_value = mask.sum()\n",
    "            kmf.fit(\n",
    "                durations=pooled_df.loc[mask, \"DSS.time\"] / 365,\n",
    "                event_observed=pooled_df.loc[mask, \"DSS\"],\n",
    "                label=f\"{risk_labels[cls]} (n={n_value})\"\n",
    "            )\n",
    "            kmf.plot_survival_function(ax=ax, ci_show=True, color=colours[cls])\n",
    "\n",
    "        ax.set_xlim(0, max_years)\n",
    "        ax.set_xlabel(\"Time (years)\", fontsize=16)\n",
    "        ax.set_ylabel(\"Survival probability\", fontsize=16)\n",
    "        ax.tick_params(axis=\"both\", labelsize=14)\n",
    "        ax.legend(fontsize=10,loc=\"lower left\",edgecolor=\"black\")\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_km_curves(survival_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f187c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_results[\"SVM\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
