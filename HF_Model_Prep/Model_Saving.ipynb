{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332c2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "\n",
    "from skorch.classifier import NeuralNetClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.special import expit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e605b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=5, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        targets = targets.view(-1,1).type_as(logits)\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "        \n",
    "# Class for the ANN model (binary classification)\n",
    "class DeepBinary(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=4, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.LazyLinear(hidden_dim))\n",
    "        layers.append(nn.LayerNorm(hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, 1))  # final logit\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Defining NeuralNet class which will be necessary for use with skorch and skopt\n",
    "class NeuralNetBinaryClassifier(NeuralNetClassifier):\n",
    "    def predict_proba(self, X):\n",
    "        logits = self.forward(X).detach().cpu().numpy()\n",
    "        probs = expit(logits)\n",
    "        return np.hstack((1 - probs, probs))\n",
    "\n",
    "_main = sys.modules.get(\"__main__\")\n",
    "_this = sys.modules.get(__name__)  # pyGSLModel.GSL_score module object\n",
    "for _name in (\"FocalLoss\", \"DeepBinary\", \"NeuralNetBinaryClassifier\"):\n",
    "    if hasattr(_this, _name) and _main is not None:\n",
    "        setattr(_main, _name, getattr(_this, _name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968ec36",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'NeuralNetBinaryClassifier' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m xgb_est \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/XGBoost.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ANN: pipeline that contains preproc + skorch classifier\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m ann_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/ANN_pipeline.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m ann_clf \u001b[38;5;241m=\u001b[39m ann_pipeline\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Ensemble artifacts\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ottoc\\Python Projects\\.conda\\Lib\\site-packages\\joblib\\numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 658\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\ottoc\\Python Projects\\.conda\\Lib\\site-packages\\joblib\\numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[0;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ottoc\\Python Projects\\.conda\\Lib\\pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\Users\\ottoc\\Python Projects\\.conda\\Lib\\pickle.py:1538\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTACK_GLOBAL requires str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ottoc\\Python Projects\\.conda\\Lib\\pickle.py:1582\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28m__import__\u001b[39m(module, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m-> 1582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_getattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)\n",
      "File \u001b[1;32mc:\\Users\\ottoc\\Python Projects\\.conda\\Lib\\pickle.py:331\u001b[0m, in \u001b[0;36m_getattribute\u001b[1;34m(obj, name)\u001b[0m\n\u001b[0;32m    329\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, subpath)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt get attribute \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m                              \u001b[38;5;241m.\u001b[39mformat(name, obj)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, parent\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'NeuralNetBinaryClassifier' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "SAVE = Path(\"re_exported_models\")\n",
    "(SAVE / \"models\").mkdir(parents=True, exist_ok=True)\n",
    "(SAVE / \"ensemble\").mkdir(parents=True, exist_ok=True)\n",
    "(SAVE / \"thresholds\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 1) load your existing artifacts (these load fine in your original env) ---\n",
    "svm_pipe = joblib.load(\"./models/SVM.pkl\")\n",
    "rf_pipe  = joblib.load(\"./models/RandomForest.pkl\")\n",
    "lr_pipe  = joblib.load(\"./models/LogisticRegression.pkl\")\n",
    "\n",
    "# XGBoost: if you previously saved via joblib, load it\n",
    "xgb_est = joblib.load(\"./models/XGBoost.pkl\")\n",
    "\n",
    "# ANN: pipeline that contains preproc + skorch classifier\n",
    "ann_pipeline = joblib.load(\"./models/ANN_pipeline.pkl\")\n",
    "ann_clf = ann_pipeline.named_steps[\"clf\"]\n",
    "\n",
    "# Ensemble artifacts\n",
    "ensemble_scaler = joblib.load(\"./ensemble/scaler.pkl\")\n",
    "beta_vec = np.load(\"./ensemble/beta_vec.npy\")\n",
    "ens_json = json.load(open(\"./ensemble/ens_threshold.json\"))\n",
    "per_model_thresholds = json.load(open(\"./thresholds/per_model_thresholds.json\"))\n",
    "\n",
    "# --- 2) Save sklearn pipelines (ok to joblib) ---\n",
    "joblib.dump(svm_pipe, SAVE / \"models/SVM_pipeline.joblib\")\n",
    "joblib.dump(rf_pipe,  SAVE / \"models/RandomForest_pipeline.joblib\")\n",
    "joblib.dump(lr_pipe,  SAVE / \"models/LogisticRegression_pipeline.joblib\")\n",
    "\n",
    "# --- 3) Save XGBoost in native format (recommended) ---\n",
    "try:\n",
    "    booster = xgb_est.get_booster()\n",
    "    booster.save_model(str(SAVE / \"models/XGBoost.json\"))\n",
    "    print(\"Saved XGBoost booster JSON.\")\n",
    "except Exception:\n",
    "    # fallback to joblib if it's not an XGBClassifier/Booster object\n",
    "    joblib.dump(xgb_est, SAVE / \"models/XGBoost.joblib\")\n",
    "    print(\"Saved XGBoost via joblib (fallback).\")\n",
    "\n",
    "# --- 4) Save ANN weights-only and preprocessor parts separately ---\n",
    "# Save skorch weights via save_params (skorch-style)\n",
    "ann_clf.save_params(f_params=str(SAVE / \"models/ANN_skorch_weights.pt\"))\n",
    "# Also save the raw module state_dict (very portable)\n",
    "torch.save(ann_clf.module_.state_dict(), SAVE / \"models/ANN_state_dict.pt\")\n",
    "\n",
    "# Save preprocessing steps from the ANN pipeline (if present)\n",
    "ann_scaler = ann_pipeline.named_steps.get(\"scaler\", None)\n",
    "ann_vt = ann_pipeline.named_steps.get(\"low_var\", None)\n",
    "if ann_scaler is not None:\n",
    "    joblib.dump(ann_scaler, SAVE / \"models/ANN_preprocessor_scaler.joblib\")\n",
    "if ann_vt is not None:\n",
    "    joblib.dump(ann_vt, SAVE / \"models/ANN_preprocessor_varthresh.joblib\")\n",
    "\n",
    "# Build and save a small curated config for the ANN (do NOT dump get_params()).\n",
    "ann_module = ann_clf.module_\n",
    "ann_config = {\n",
    "    \"module_class\": \"DeepBinary\",   # document the class name you will import in your package\n",
    "    \"module_args\": {\n",
    "        # These attributes might not exist on module_; fill in if you used them when creating it\n",
    "        \"hidden_dim\": getattr(ann_module, \"hidden_dim\", 117),\n",
    "        \"num_layers\": getattr(ann_module, \"num_layers\", 1),\n",
    "        \"dropout_rate\": getattr(ann_module, \"dropout_rate\", 0.32177566932368323),\n",
    "    },\n",
    "    \"skorch\": {\n",
    "        \"lr\": float(getattr(ann_clf, \"lr\", 1e-3)),\n",
    "        \"max_epochs\": int(getattr(ann_clf, \"max_epochs\", 100)),\n",
    "        \"batch_size\": int(getattr(ann_clf, \"batch_size\", 128)),\n",
    "        \"criterion\": ann_clf.criterion.__class__.__name__ if hasattr(ann_clf, \"criterion\") else None\n",
    "    },\n",
    "    \"notes\": \"Recreate DeepBinary with module_args and then load ANN_state_dict.pt\"\n",
    "}\n",
    "json.dump(ann_config, open(SAVE / \"models/ANN_config.json\", \"w\"), indent=2)\n",
    "\n",
    "# --- 5) Save ensemble artifacts (scaler, beta_vec, thresholds) ---\n",
    "joblib.dump(ensemble_scaler, SAVE / \"ensemble/scaler.joblib\")\n",
    "np.save(SAVE / \"ensemble/beta_vec.npy\", beta_vec)\n",
    "json.dump(ens_json, open(SAVE / \"ensemble/ens_threshold.json\", \"w\"), indent=2)\n",
    "json.dump(per_model_thresholds, open(SAVE / \"thresholds/per_model_thresholds.json\", \"w\"), indent=2)\n",
    "\n",
    "# --- 6) Save a small environment manifest to help future reproducibility ---\n",
    "import platform, pkg_resources\n",
    "env_info = {\n",
    "    \"python\": platform.python_version(),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"packages\": {p.key: p.version for p in pkg_resources.working_set\n",
    "                 if p.key in (\"torch\", \"skorch\", \"scikit-learn\", \"xgboost\", \"numpy\", \"pandas\", \"joblib\")}\n",
    "}\n",
    "json.dump(env_info, open(SAVE / \"metadata_env.json\", \"w\"), indent=2)\n",
    "\n",
    "print(\"Re-export complete into:\", SAVE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567d5a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 117, 'num_layers': 1, 'dropout_rate': 0.32177566932368323}\n"
     ]
    }
   ],
   "source": [
    "# ANN: pipeline that contains preproc + skorch classifier\n",
    "ann = joblib.load(\"./models/ANN_pipeline.pkl\")\n",
    "ann.named_steps[\"clf\"].load_params(f_params=\"./models/ANN_params.pt\")\n",
    "ann_clf = ann.named_steps[\"clf\"]\n",
    "ann_module = ann_clf.module_\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def extract_deepbinary_architecture(module: nn.Module):\n",
    "    # hidden_dim: from LayerNorm or Linear weights\n",
    "    hidden_dim = None\n",
    "    dropout_rate = None\n",
    "    linear_layers = 0\n",
    "\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.LayerNorm) and hidden_dim is None:\n",
    "            # LayerNorm(normalized_shape=(hidden_dim,))\n",
    "            hidden_dim = int(m.normalized_shape[0])\n",
    "\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # count all linear layers\n",
    "            linear_layers += 1\n",
    "\n",
    "        if isinstance(m, nn.Dropout) and dropout_rate is None:\n",
    "            dropout_rate = float(m.p)\n",
    "\n",
    "    if hidden_dim is None:\n",
    "        raise RuntimeError(\"Could not infer hidden_dim from trained ANN\")\n",
    "\n",
    "    if dropout_rate is None:\n",
    "        dropout_rate = 0.25  # safe fallback\n",
    "\n",
    "    # final Linear(hidden_dim → 1) → num_layers = total - 1\n",
    "    num_layers = max(1, linear_layers - 1)\n",
    "\n",
    "    return {\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "    }\n",
    "\n",
    "arch = extract_deepbinary_architecture(ann_module)\n",
    "print(arch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
