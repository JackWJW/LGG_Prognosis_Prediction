{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332c2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "\n",
    "from skorch.classifier import NeuralNetClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.special import expit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2e605b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=5, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        targets = targets.view(-1,1).type_as(logits)\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "        \n",
    "# Class for the ANN model (binary classification)\n",
    "class DeepBinary(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=4, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.LazyLinear(hidden_dim))\n",
    "        layers.append(nn.LayerNorm(hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, 1))  # final logit\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Defining NeuralNet class which will be necessary for use with skorch and skopt\n",
    "class NeuralNetBinaryClassifier(NeuralNetClassifier):\n",
    "    def predict_proba(self, X):\n",
    "        logits = self.forward(X).detach().cpu().numpy()\n",
    "        probs = expit(logits)\n",
    "        return np.hstack((1 - probs, probs))\n",
    "\n",
    "_main = sys.modules.get(\"__main__\")\n",
    "_this = sys.modules.get(__name__)  # pyGSLModel.GSL_score module object\n",
    "for _name in (\"FocalLoss\", \"DeepBinary\", \"NeuralNetBinaryClassifier\"):\n",
    "    if hasattr(_this, _name) and _main is not None:\n",
    "        setattr(_main, _name, getattr(_this, _name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3968ec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XGBoost via joblib (fallback).\n",
      "Re-export complete into: re_exported_models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ottoc\\AppData\\Local\\Temp\\ipykernel_18816\\3607545328.py:80: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import platform, pkg_resources\n"
     ]
    }
   ],
   "source": [
    "SAVE = Path(\"re_exported_models\")\n",
    "(SAVE / \"models\").mkdir(parents=True, exist_ok=True)\n",
    "(SAVE / \"ensemble\").mkdir(parents=True, exist_ok=True)\n",
    "(SAVE / \"thresholds\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 1) load your existing artifacts (these load fine in your original env) ---\n",
    "svm_pipe = joblib.load(\"./models/SVM.pkl\")\n",
    "rf_pipe  = joblib.load(\"./models/RandomForest.pkl\")\n",
    "lr_pipe  = joblib.load(\"./models/LogisticRegression.pkl\")\n",
    "\n",
    "# XGBoost: if you previously saved via joblib, load it\n",
    "xgb_est = joblib.load(\"./models/XGBoost.pkl\")\n",
    "\n",
    "# ANN: pipeline that contains preproc + skorch classifier\n",
    "ann_pipeline = joblib.load(\"./models/ANN_pipeline.pkl\")\n",
    "ann_clf = ann_pipeline.named_steps[\"clf\"]\n",
    "\n",
    "# Ensemble artifacts\n",
    "ensemble_scaler = joblib.load(\"./ensemble/scaler.pkl\")\n",
    "beta_vec = np.load(\"./ensemble/beta_vec.npy\")\n",
    "ens_json = json.load(open(\"./ensemble/ens_threshold.json\"))\n",
    "per_model_thresholds = json.load(open(\"./thresholds/per_model_thresholds.json\"))\n",
    "\n",
    "# --- 2) Save sklearn pipelines (ok to joblib) ---\n",
    "joblib.dump(svm_pipe, SAVE / \"models/SVM_pipeline.joblib\")\n",
    "joblib.dump(rf_pipe,  SAVE / \"models/RandomForest_pipeline.joblib\")\n",
    "joblib.dump(lr_pipe,  SAVE / \"models/LogisticRegression_pipeline.joblib\")\n",
    "\n",
    "# --- 3) Save XGBoost in native format (recommended) ---\n",
    "try:\n",
    "    booster = xgb_est.get_booster()\n",
    "    booster.save_model(str(SAVE / \"models/XGBoost.json\"))\n",
    "    print(\"Saved XGBoost booster JSON.\")\n",
    "except Exception:\n",
    "    # fallback to joblib if it's not an XGBClassifier/Booster object\n",
    "    joblib.dump(xgb_est, SAVE / \"models/XGBoost.joblib\")\n",
    "    print(\"Saved XGBoost via joblib (fallback).\")\n",
    "\n",
    "# --- 4) Save ANN weights-only and preprocessor parts separately ---\n",
    "# Save skorch weights via save_params (skorch-style)\n",
    "ann_clf.save_params(f_params=str(SAVE / \"models/ANN_skorch_weights.pt\"))\n",
    "# Also save the raw module state_dict (very portable)\n",
    "torch.save(ann_clf.module_.state_dict(), SAVE / \"models/ANN_state_dict.pt\")\n",
    "\n",
    "# Save preprocessing steps from the ANN pipeline (if present)\n",
    "ann_scaler = ann_pipeline.named_steps.get(\"scaler\", None)\n",
    "ann_vt = ann_pipeline.named_steps.get(\"low_var\", None)\n",
    "if ann_scaler is not None:\n",
    "    joblib.dump(ann_scaler, SAVE / \"models/ANN_preprocessor_scaler.joblib\")\n",
    "if ann_vt is not None:\n",
    "    joblib.dump(ann_vt, SAVE / \"models/ANN_preprocessor_varthresh.joblib\")\n",
    "\n",
    "# Build and save a small curated config for the ANN (do NOT dump get_params()).\n",
    "ann_module = ann_clf.module_\n",
    "ann_config = {\n",
    "    \"module_class\": \"DeepBinary\",   # document the class name you will import in your package\n",
    "    \"module_args\": {\n",
    "        # These attributes might not exist on module_; fill in if you used them when creating it\n",
    "        \"hidden_dim\": getattr(ann_module, \"hidden_dim\", None),\n",
    "        \"num_layers\": getattr(ann_module, \"num_layers\", None),\n",
    "        \"dropout_rate\": getattr(ann_module, \"dropout_rate\", None),\n",
    "    },\n",
    "    \"skorch\": {\n",
    "        \"lr\": float(getattr(ann_clf, \"lr\", 1e-3)),\n",
    "        \"max_epochs\": int(getattr(ann_clf, \"max_epochs\", 100)),\n",
    "        \"batch_size\": int(getattr(ann_clf, \"batch_size\", 128)),\n",
    "        \"criterion\": ann_clf.criterion.__class__.__name__ if hasattr(ann_clf, \"criterion\") else None\n",
    "    },\n",
    "    \"notes\": \"Recreate DeepBinary with module_args and then load ANN_state_dict.pt\"\n",
    "}\n",
    "json.dump(ann_config, open(SAVE / \"models/ANN_config.json\", \"w\"), indent=2)\n",
    "\n",
    "# --- 5) Save ensemble artifacts (scaler, beta_vec, thresholds) ---\n",
    "joblib.dump(ensemble_scaler, SAVE / \"ensemble/scaler.joblib\")\n",
    "np.save(SAVE / \"ensemble/beta_vec.npy\", beta_vec)\n",
    "json.dump(ens_json, open(SAVE / \"ensemble/ens_threshold.json\", \"w\"), indent=2)\n",
    "json.dump(per_model_thresholds, open(SAVE / \"thresholds/per_model_thresholds.json\", \"w\"), indent=2)\n",
    "\n",
    "# --- 6) Save a small environment manifest to help future reproducibility ---\n",
    "import platform, pkg_resources\n",
    "env_info = {\n",
    "    \"python\": platform.python_version(),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"packages\": {p.key: p.version for p in pkg_resources.working_set\n",
    "                 if p.key in (\"torch\", \"skorch\", \"scikit-learn\", \"xgboost\", \"numpy\", \"pandas\", \"joblib\")}\n",
    "}\n",
    "json.dump(env_info, open(SAVE / \"metadata_env.json\", \"w\"), indent=2)\n",
    "\n",
    "print(\"Re-export complete into:\", SAVE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
